1.Program to implement K Nearest Neighbor classifier and Regressor and also  
evaluate the model. 

A)REGRESSION (HOUSE-PRICE): 

import pandas as pd
import numpy as np
df = pd.read_csv('/Housing.csv')
from sklearn.preprocessing import StandardScaler
cat_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating',
            'airconditioning', 'prefarea', 'furnishingstatus']
df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)
x = df_encoded.drop('price', axis=1)
y = df_encoded['price']
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
from sklearn.neighbors import KNeighborsRegressor
model = KNeighborsRegressor(n_neighbors=4)
model.fit(x_train, y_train)
sample_input = [7420, 4, 2, 3, 2,  
                1, 0, 0, 0, 1, 1,  
                0, 1]                      
sample_scaled = scaler.transform([sample_input])
predicted_price = model.predict(sample_scaled)
print("Predicted Price:", predicted_price[0])
y_pred = model.predict(x_test)
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R2 Score: {r2:.4f}")

-------------------------------------------

B)CLASSIFICATION(TITANIC)

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
df = pd.read_csv("/Titanic-Dataset.csv")
df['Age'] = df['Age'].fillna(df['Age'].mean())
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])
le_sex = LabelEncoder()
df['Sex'] = le_sex.fit_transform(df['Sex'])
le_embarked = LabelEncoder()
df['Embarked'] = le_embarked.fit_transform(df['Embarked'])
X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]
y = df['Survived']
scaler = StandardScaler()
X = scaler.fit_transform(X)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
model.fit(X_train, y_train)
sample = [[3, 1, 22, 1, 0, 7.25, 2]]
prediction = model.predict(sample)
print(f"Prediction for sample {sample}: {prediction[0]}")
y_pred = model.predict(X_test)
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, average='micro')
rec = recall_score(y_test, y_pred, average='micro')
f1 = f1_score(y_test, y_pred, average='micro')
print(f"\nAccuracy: {acc}")
print(f"Precision: {prec}")
print(f"Recall: {rec}")
print(f"F1 Score: {f1}")

-------------------------------------------------------------------------------------------

2.Develop a program for handling missing values in diabetes dataset using KNN imputer

import pandas as pd       
import numpy as np        
from sklearn.impute import KNNImputer   # KNN Imputer

df = pd.read_csv("diabetes.csv")

# Columns where zero means missing values
zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

df[zero_cols] = df[zero_cols].replace(0, np.nan)

print("Before Imputation:")
print(df.isnull().sum())   # Show number of missing values

imputer = KNNImputer(n_neighbors=5)
df_imputed = imputer.fit_transform(df)

df_imputed = pd.DataFrame(df_imputed, columns=df.columns)

print("\nAfter Imputation:")
print(df_imputed.isnull().sum())

df_imputed.to_csv("diabetes_cleaned.csv", index=False)
print("\nImputed dataset saved as diabetes_cleaned.csv")

----------------------------------------------------------------------------------------------
3.Analyze the sample data by plo ng uni-variate plots(histograms, density plots, box plots 
and whisker plots) 

#HISTOGRAMS(DIABETES)

import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv("diabetes.csv")
print(df.head())
print(df.shape)
df.hist(figsize=(12, 10))
plt.tight_layout()
plt.show()
#DENSITY PLOT(DIABETES)
df['BloodPressure'].plot(kind='density', figsize=(8,5), color='blue')
plt.title("Density Plot of BloodPressure")
plt.xlabel("BloodPressure")
plt.show()
#BOXPLOT OF GLUCOSE(DIABETES)
plt.boxplot(df['Glucose'])
plt.title('Box plot of glucose')
plt.ylabel('Glucose level')
plt.show()
#HEATMAP(DIABETES)
pearson_corr = df.corr(method='pearson')
plt.figure(figsize=(10, 8))
plt.imshow(pearson_corr, cmap='coolwarm', interpolation='nearest')
plt.colorbar(label='Correlation coefficient')
plt.xticks(range(len(pearson_corr.columns)), pearson_corr.columns, rotation=90)
plt.yticks(range(len(pearson_corr.columns)), pearson_corr.columns)
plt.title("Pearson correlation heatmap - Pima diabetes dataset")
plt.show()
#SCATTER MATRIX(DIABETES)
from pandas.plotting import scatter_matrix
plt.rcParams['figure.figsize'] = [20, 20]
scatter_matrix(df)
plt.show()

---------------------------------------------------------------------

#IRIS DATASET

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from pandas.plotting import scatter_matrix
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = iris.target
df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})
print(df.head())
print(df.shape)
# Plot histograms for all features
df.hist(figsize=(12, 10), edgecolor='black')
plt.suptitle("Histograms of Iris Dataset Features", fontsize=14)
plt.tight_layout()
plt.show()
# Density plot for Sepal Length
df['sepal length (cm)'].plot(kind='density', figsize=(8,5), color='blue')
plt.title("Density Plot of Sepal Length")
plt.xlabel("Sepal Length (cm)")
plt.show()
# Box plot for Sepal Length
plt.boxplot(df['sepal length (cm)'])
plt.title('Box Plot of Sepal Length')
plt.ylabel('Sepal Length (cm)')
plt.show()
# Scatter matrix plot
plt.rcParams['figure.figsize'] = [20, 20]
scatter_matrix(df, alpha=0.8, diagonal='hist')
plt.suptitle("Scatter Matrix of Iris Dataset")
plt.show()

#CORRELATION MATRIX (IRIS)
import pandas as pd          
from sklearn.datasets import load_iris  
import seaborn as sns        
import matplotlib.pyplot as plt  
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names) 
corr = df.corr()
plt.figure(figsize=(8, 6)) 
sns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=0.5) 
plt.title("Correlation Matrix - Iris Dataset")  
plt.show()  

---------------------------------------------------------------------------

4.Program to implement Principle Component Analysis on sample dataset.

A)PCA ON IRIS 

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# Load dataset
iris = load_iris()
x = iris.data
y = iris.target
target_names = iris.target_names

# Perform PCA (2 components)
pca = PCA(n_components=2)
x_pca = pca.fit_transform(x)

# Create DataFrame with PCA results
df_pca = pd.DataFrame(data=x_pca, columns=['PCA1', 'PCA2'])
df_pca['target'] = [target_names[i] for i in y]

# Print first 5 rows
print(df_pca.head(5))

# Print explained variance values
print("\n Explained Variance:", pca.explained_variance_)
print("Explained Variance Ratio:", pca.explained_variance_ratio_)
print("Cumulative variance explained:", pca.explained_variance_ratio_.sum())

--------------------------------------

B)PCA ON GIVEN DATASET

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Input data
x = np.array([
    [2.5, 2.4, 0.5, 1.2],
    [0.5, 0.7, 1.5, 0.9],
    [2.2, 2.9, 0.3, 1.3],
    [1.9, 2.2, 0.4, 1.0],
    [3.1, 3.0, 0.2, 1.4],
    [2.3, 2.7, 0.7, 1.1]
], dtype=float)

# Number of components
k = 2
pca = PCA(n_components=k)

# Fit and transform
x_pca = pca.fit_transform(x)

print("Original Shape:", x.shape)
print("Transformed Shape:", x_pca.shape)

print("\nTransformed Data (PCA result):\n", x_pca)
print("\nExplained Variance:", pca.explained_variance_)
print("Explained Variance Ratio:", pca.explained_variance_ratio_)
print("Cumulative Variance Explained:", np.cumsum(pca.explained_variance_ratio_))
print("\nPrincipal Components (Eigenvectors):\n", pca.components_)

-------------------------------------------------------------------------------

5.DECISION TREE

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris

# Load dataset
df = load_iris()
x = df.data
y = df.target

# Train-test split
from sklearn.model_selection import train_test_split
a, b, c, d = train_test_split(x, y, test_size=0.2, random_state=42)

# Decision Tree model
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(max_depth=6)
clf.fit(a, c)

# Predictions
y_pred = clf.predict(b)

# Evaluation metrics
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score

print("Confusion Matrix:\n", confusion_matrix(d, y_pred))
print("precision:", precision_score(d, y_pred, average='weighted'))
print("accuracy:", accuracy_score(d, y_pred))
print("f1_score:", f1_score(d, y_pred, average='weighted'))
print("recall_score:", recall_score(d, y_pred, average='weighted'))

----------------------------------------------------------------------------------

6.APRIORI

!pip install mlxtend --quiet

import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
dataset = [
    ['milk', 'jam'],
    ['milk', 'bread', 'jam'],
    ['milk', 'butter'],
    ['milk', 'bread', 'butter'],
    ['bread', 'butter']
]
te = TransactionEncoder()
te_ary = te.fit(dataset).transform(dataset)
df = pd.DataFrame(te_ary, columns=te.columns_)
print("\nOne-Hot Encoded DataFrame:\n")
print(df)
frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)
print("\nFrequent Itemsets:\n")
print(frequent_itemsets)
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.6)
print("\nAssociation Rules:\n")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

-------------------------------------------------------------------------------------------

7.Program to implement perceptron for different learning task and evaluate the  
algorithm.(ANN-Diabetes dataset).

!pip install tensorflow --quiet

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
df = pd.read_csv('/content/drive/MyDrive/Copy of diabetes.csv')
X = df.iloc[:, :-1]     
y = df.iloc[:, -1]      
y = keras.utils.to_categorical(y, num_classes=2)

a, c, b, d = train_test_split(X, y, test_size=0.2, random_state=42)
model = Sequential()
model.add(Dense(32, activation='relu', input_dim=8))
model.add(Dense(16, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.summary()
model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])
model.fit(a, c, validation_data=(b, d), epochs=100, batch_size=20)

------------------------------------------------------------------------------

8.LINEAR REGRESSION 

A)salary-datacsv

# Simple Linear Regression
import pandas as pd
df = pd.read_csv('salary_data.csv')
X = df.iloc[:, 0]
Y = df.iloc[:, -1]
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X.values.reshape(-1, 1), Y)
print(regressor.score(X.values.reshape(-1, 1), Y))
print(regressor.coef_)
print(regressor.intercept_)
import matplotlib.pyplot as plt
plt.scatter(X, Y)
plt.plot(X, regressor.predict(X.values.reshape(-1, 1)))
plt.show()
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
print(mean_absolute_error(Y, regressor.predict(X.values.reshape(-1, 1))))
print(mean_squared_error(Y, regressor.predict(X.values.reshape(-1, 1))))
print(r2_score(Y, regressor.predict(X.values.reshape(-1, 1))))
print(regressor.predict([[5]]))

-----------------------------------

B)Housing Prices Dataset

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
df = pd.read_csv("housing.csv")   # change file name if needed
X = df.drop(['price'], axis=1)
y = df['price']
numeric_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']
binary_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating',
                   'airconditioning', 'prefarea']
multi_cat_features = ['furnishingstatus']
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numeric_features),
    ('bin', OneHotEncoder(drop='first'), binary_features),
    ('multi', OneHotEncoder(drop='first'), multi_cat_features)
])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)
model = LinearRegression()
model.fit(X_train_processed, y_train)
y_pred = model.predict(X_test_processed)
print("Mean Absolute Error:", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("R2 Score:", r2_score(y_test, y_pred))
print("\nIntercept:", model.intercept_)
print("Number of Coefficients:", len(model.coef_))
print("\nCoefficients:\n", model.coef_)

----------------------------------------------------------------------------------

9.Program to implement KMeans algorithm and evaluate the algorithm.

#line graph

import os
os.environ["OMP_NUM_THREADS"] = "1"   # To avoid thread warnings

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

# -----------------------
# Load Dataset
# -----------------------
df = pd.read_csv('Mall_Customers.csv')

# -----------------------
# Elbow Method to find optimal clusters
# -----------------------
wcss = []

for i in range(1, 11):
    km = KMeans(n_clusters=i, n_init='auto', random_state=42)
    km.fit(df[['Annual Income (k$)', 'Spending Score (1-100)']])
    wcss.append(km.inertia_)

# -----------------------
# Plot Elbow Curve
# -----------------------
plt.figure(figsize=(6,4))
plt.plot(range(1, 11), wcss, marker='o')
plt.title("Elbow Method")
plt.xlabel("Number of Clusters")
plt.ylabel("WCSS")
plt.show()

-------------------------------------------------------------

#scatter plot

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Select features
X = df[['Annual Income (k$)', 'Spending Score (1-100)']]

# Build KMeans Model
km = KMeans(n_clusters=5, n_init='auto', random_state=42)
km.fit(X)

# Add cluster labels to DataFrame
df['label'] = km.fit_predict(X)

# Plotting
plt.figure(figsize=(6,4))
sns.scatterplot(
    x='Annual Income (k$)',
    y='Spending Score (1-100)',
    hue='label',
    palette='rainbow',
    data=df,
    s=60
)

plt.title('Annual Income vs Spending Score')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.show()

------------------------------------------------------------

#3

cust1 = df[df['label'] == 1]
print('Number of customer in 1st group=', len(cust1))
print('They are - ', cust1["CustomerID"].values)
print("----------------------------------------------")

cust2 = df[df['label'] == 2]
print('Number of customer in 2nd group=', len(cust2))
print('They are - ', cust2["CustomerID"].values)
print("----------------------------------------------")

cust3 = df[df['label'] == 3]
print('Number of customer in 3rd group=', len(cust3))
print('They are - ', cust3["CustomerID"].values)
print("----------------------------------------------")

cust4 = df[df['label'] == 4]
print('Number of customer in 4th group=', len(cust4))
print('They are - ', cust4["CustomerID"].values)
print("----------------------------------------------")

cust5 = df[df['label'] == 0]
print('Number of customer in 5th group=', len(cust5))
print('They are - ', cust5["CustomerID"].values)
print("----------------------------------------------")


i]);
remaining[i] = burst[i];
}
int current_time = 0;
int completed = 0;
// Gantt chart storage
int gantt_pid[1000], gantt_time[1000];
int g_index = 0;
// Round Robin logic
while (completed != n) {
int done_anything = 0;
for (int i = 0; i < n; i++) {
if (arrival[i] <= current_time && remaining[i] > 0) {
done_anything = 1;
gantt_pid[g_index] = pid[i]; // record process execution
gantt_time[g_index] = current_time;
g_index++;

if (remaining[i] > quantum) {
remaining[i] -= quantum;
current_time += quantum;
} else {
current_time += remaining[i];
remaining[i] = 0;
completion[i] = current_time;
completed++;
}
}
}
// If CPU had no job at this time
if (!done_anything) {
current_time++;
}
}
// Calculate WT and TAT
for (int i = 0; i < n; i++) {
turnaround[i] = completion[i] - arrival[i];
waiting[i] = turnaround[i] - burst[i];
}
// Display table
printf("\nProcess\tAT\tBT\tWT\tTAT\n");
for (int i = 0; i < n; i++) {
printf("P%d\t%d\t%d\t%d\t%d\n",
pid[i], arrival[i], burst[i], waiting[i], turnaround[i]);
}
// Print Gantt chart
printf("\n\nGantt Chart:\n");
printf("--------------------------------------------------\n");
for (int i = 0; i < g_index; i++) {
printf(" P%d |", gantt_pid[i]);
}
printf("\n--------------------------------------------------\n");
// Print timeline
for (int i = 0; i < g_index; i++) {
printf("%d ", gantt_time[i]);
}
printf("%d\n", current_time);
return 0;
}
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Lab-5
----------------------Semaphores------------------------------#include
#include

#include
#include
#define MAX_PORTS 5 // maximum number of threads allowed at once
sem_t port_slots; // semaphore variable
// Thread function
void* worker(void *arg) {
// Each thread tries to acquire a port slot
sem_wait(&port_slots); // wait (decrease semaphore), block if 0
// Critical section - only MAX_PORTS threads can be here together
printf("Thread %ld: opened a port\n", (long)arg);
sleep(1); // simulate work
printf("Thread %ld: closing port\n", (long)arg);
// Release the port slot
sem_post(&port_slots); // increase semaphore (free resource)
return NULL;
}
int main() {
// Initialise the semaphore with MAX_PORTS slots
// 0 means: semaphore shared between threads in this process
sem_init(&port_slots, 0, MAX_PORTS);
pthread_t th[20]; // array for 20 threads
// Create 20 threads
for (long i = 0; i < 20; i++) {
pthread_create(&th[i], NULL, worker, (void*)i);
}
// Wait for all threads to complete
for (int i = 0; i < 20; i++) {
pthread_join(th[i], NULL);
}
// Destroy semaphore
sem_destroy(&port_slots);
return 0;
}
----------------------Monitors------------------------------#include
#include
#include
#define MAX_PORTS 5 // Max threads allowed inside critical area at once
int openPorts = 0; // Currently active "ports"

pthread_mutex_t lock; // Monitor lock
pthread_cond_t cond; // Condition variable
void* worker(void *arg) {
// Enter monitor
pthread_mutex_lock(&lock);
// If all ports are full, wait
while (openPorts == MAX_PORTS) {
pthread_cond_wait(&cond, &lock);
}
// Now this thread can take a port
openPorts++;
printf("Thread %ld: opened a port (open = %d)\n", (long)arg, openPorts);
// Leave monitor (allow others to check capacity)
pthread_mutex_unlock(&lock);
// Simulate using the port
sleep(1);
// Re-enter monitor to release the port
pthread_mutex_lock(&lock);
openPorts--;
printf("Thread %ld: closing port (open = %d)\n", (long)arg, openPorts);
// Wake one waiting thread
pthread_cond_signal(&cond);
// Exit monitor
pthread_mutex_unlock(&lock);
return NULL;
}
int main() {
pthread_t th[20];
// Initialise mutex & condition variable
pthread_mutex_init(&lock, NULL);
pthread_cond_init(&cond, NULL);
// Create 20 threads
for (long i = 0; i < 20; i++) {
pthread_create(&th[i], NULL, worker, (void*)i);
}
// Wait for all threads to complete
for (int i = 0; i < 20; i++) {
pthread_join(th[i], NULL);

}
// Destroy the monitor components
pthread_mutex_destroy(&lock);
pthread_cond_destroy(&cond);
return 0;
}
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Lab-6
----------------------Concurrent Threads------------------------------#include
#include
#include // The POSIX Threads library
#include // For usleep()
#define NUM_THREADS 5 // We'll use 5 threads
// This is the function that all threads will run
void* thread_function(void *arg) {
// Get the message passed to this thread
char *message = (char*) arg;
for (int i = 0; i < 10; i++) {
// Print the thread's message
printf("%s\n", message);
// Pause briefly to encourage the OS to switch threads
usleep(1);
}
return NULL;
}
int main() {
// Create an array to hold the 5 thread IDs
pthread_t thread_ids[NUM_THREADS];
// Create an array of messages, one for each thread
char *messages[NUM_THREADS] = {
"Thread 1: AAAAAAAAAA",
"Thread 2: BBBBBBBBBB",
"Thread 3: CCCCCCCCCC",
"Thread 4: DDDDDDDDDD",
"Thread 5: EEEEEEEEEE"
};
printf("Starting %d threads...\n", NUM_THREADS);
// --- 1. Create all 5 threads in a loop --for (int i = 0; i < NUM_THREADS; i++) {
if (pthread_create(&thread_ids[i], NULL, thread_function, (void*)messages[i]) != 0) {
perror("Failed to create thread");
return 1;

}
}
// --- 2. Wait for all 5 threads to finish --printf("Waiting for threads to join...\n");
for (int i = 0; i < NUM_THREADS; i++) {
pthread_join(thread_ids[i], NULL);
}

printf("All %d threads have finished execution.\n", NUM_THREADS);
return 0;
}
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Lab-7
-----------------------------producer-consumer--------------------------------int main() {
pthread_t prod, cons; // Variables to hold the thread IDs
// Initialise semaphores and mutex
// sem_init(sem, pshared, value)
// pshared=0 means shared between threads
sem_init(&empty, 0, BUFFER_SIZE); // Init 'empty' count to 5
sem_init(&full, 0, 0); // Init 'full' count to 0
pthread_mutex_init(&mutex, NULL); // Init the mutex lock (default attributes)
// Create producer and consumer threads
pthread_create(&prod, NULL, producer, NULL); // Start producer thread
pthread_create(&cons, NULL, consumer, NULL); // Start consumer thread
// Wait for threads to finish (which they never will)
// The main thread will block here forever
pthread_join(prod, NULL);
pthread_join(cons, NULL);
// Clean up (this code is unreachable but good practice)
sem_destroy(&empty);
sem_destroy(&full);
pthread_mutex_destroy(&mutex);
return 0;
}
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Lab-8
----------------------first Fit------------------------------#include
int main() {
int nb, np, b[10], p[10], allocation[10];
int i, j;
printf("Enter the number of memory blocks: ");
scanf("%d", &nb);

printf("Enter the size of each block:\n");
for (i = 0; i < nb; i++) {
printf("Block %d: ", i + 1);
scanf("%d", &b[i]);
}
printf("Enter the number of processes: ");
scanf("%d", &np);
printf("Enter the size of each process:\n");
for (i = 0; i < np; i++) {
printf("Process %d: ", i + 1);
scanf("%d", &p[i]);
}
// Initialize allocation array (-1 means not allocated)
for (i = 0; i < np; i++)
allocation[i] = -1;
// First Fit Allocation
for (i = 0; i < np; i++) {
for (j = 0; j < nb; j++) {
if (b[j] >= p[i]) {
allocation[i] = j;
b[j] -= p[i]; // Reduce the available size of the block
break;
}
}
}
// Display allocation results
printf("\nProcess No.\tProcess Size\tBlock No.\n");
for (i = 0; i < np; i++) {
if (allocation[i] != -1)
printf("%d\t\t%d\t\t%d\n", i + 1, p[i], allocation[i] + 1);
else
printf("%d\t\t%d\t\tNot Allocated\n", i + 1, p[i]);
}
return 0;
}
----------------------worst Fit------------------------------#include
int main() {
int nb, np, b[10], p[10], allocation[10];
int i, j, worst;
printf("Enter the number of memory blocks: ");
scanf("%d", &nb);
printf("Enter the size of each block:\n");
for (i = 0; i < nb; i++) {
printf("Block %d: ", i + 1);
scanf("%d", &b[i]);
}
printf("Enter the number of processes: ");
scanf("%d", &np);
printf("Enter the size of each process:\n");
for (i = 0; i < np; i++) {
printf("Process %d: ", i + 1);
scanf("%d", &p[i]);
}

// Initialize allocation array (-1 means not allocated)
for (i = 0; i < np; i++)
allocation[i] = -1;
// Worst Fit Allocation
for (i = 0; i < np; i++) {
worst = -1;
for (j = 0; j < nb; j++) {
if (b[j] >= p[i]) { // block can fit process
if (worst == -1 || b[j] > b[worst])
worst = j; // choose the largest suitable block
}
}
if (worst != -1) { // allocate process
allocation[i] = worst;
b[worst] -= p[i]; // reduce block size
}
}
// Display allocation results
printf("\nProcess No.\tProcess Size\tBlock No.\n");
for (i = 0; i < np; i++) {
if (allocation[i] != -1)
printf("%d\t\t%d\t\t%d\n", i + 1, p[i], allocation[i] + 1);
else
printf("%d\t\t%d\t\tNot Allocated\n", i + 1, p[i]);
}
return 0;
}
----------------------Best Fit------------------------------#include
int main() {
int nb, np, b[10], p[10], allocation[10];
int i, j, best;
printf("Enter the number of memory blocks: ");
scanf("%d", &nb);
printf("Enter the size of each block:\n");
for (i = 0; i < nb; i++) {
printf("Block %d: ", i + 1);
scanf("%d", &b[i]);
}
printf("Enter the number of processes: ");
scanf("%d", &np);
printf("Enter the size of each process:\n");
for (i = 0; i < np; i++) {
printf("Process %d: ", i + 1);
scanf("%d", &p[i]);
}
// Initialize allocation array
for (i = 0; i < np; i++)
allocation[i] = -1;
// Best Fit Allocation
for (i = 0; i < np; i++) {
best = -1;
for (j = 0; j < nb; j++) {
if (b[j] >= p[i]) { // block can fit process

if (best == -1 || b[j] < b[best])
best = j; // choose the smaller suitable block
}
}
